{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "# from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "# from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "# from keras.models import Model\n",
    "# from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the data folder location\n",
    "BASE_DIR = ''\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'data/20_newsgroup')\n",
    "\n",
    "# EMBEDDING_DIR = os.path.join(BASE_DIR, '../../../../tools-softwares/wordvectors/')\n",
    "# EMBEDDING_FILE_NAME = 'glove.6B.50d.txt'\n",
    "# EMBEDDING_DIM = 50\n",
    "\n",
    "# MAX_SEQUENCE_LENGTH = 1000\n",
    "# MAX_NUM_WORDS = 20000\n",
    "\n",
    "# VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 19997 texts.\n"
     ]
    }
   ],
   "source": [
    "# prepare text samples and their labels\n",
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index)\n",
    "        labels_index[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                if sys.version_info < (3,):\n",
    "                    f = open(fpath)\n",
    "                else:\n",
    "                    f = open(fpath, encoding='latin-1')\n",
    "            t = f.read()\n",
    "            i = t.find('\\n\\n')  # skip header\n",
    "            if 0 < i:\n",
    "                t = t[i:]\n",
    "            texts.append(t)\n",
    "            f.close()\n",
    "            labels.append(label_id)\n",
    "\n",
    "print('Found %s texts.' % len(texts))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16829</th>\n",
       "      <td>\\n\\nIn article &lt;1993Apr7.141930.29582@freenet....</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9599</th>\n",
       "      <td>\\n\\nIn article &lt;1993Apr20.013653.1@eagle.wesle...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19830</th>\n",
       "      <td>\\n\\nunfortunately not\\n\\n\\n--\\nLegalize Freedom\\n</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14278</th>\n",
       "      <td>\\n\\nIn &lt;19APR199320262420@kelvin.jpl.nasa.gov&gt;...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640</th>\n",
       "      <td>\\n\\nAdam Benson\\nMt. Pearl, NF\\nadamb@garfield...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  label\n",
       "16829  \\n\\nIn article <1993Apr7.141930.29582@freenet....     16\n",
       "9599   \\n\\nIn article <1993Apr20.013653.1@eagle.wesle...      9\n",
       "19830  \\n\\nunfortunately not\\n\\n\\n--\\nLegalize Freedom\\n     19\n",
       "14278  \\n\\nIn <19APR199320262420@kelvin.jpl.nasa.gov>...     14\n",
       "2640   \\n\\nAdam Benson\\nMt. Pearl, NF\\nadamb@garfield...      2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the input data frame\n",
    "data = pd.DataFrame({'document': texts, 'label': labels})\n",
    "data = data.sample(frac=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min tweet len: 0\n",
      "max tweet len: 11782\n",
      "avg tweet len: 262.56478471770765\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see the distribution of the document length\n",
    "doc_len = [len(s.split()) for s in data['document']]\n",
    "print('min tweet len: ' + str(np.min(doc_len)))\n",
    "print('max tweet len: ' + str(np.max(doc_len)))\n",
    "print('avg tweet len: ' + str(np.mean(doc_len)))\n",
    "plt.hist(doc_len, bins='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the documents and labels in numpy array\n",
    "documents = data['document'].values\n",
    "y = data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_train, documents_test, y_train, y_test = train_test_split(documents, y, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 141598\n"
     ]
    }
   ],
   "source": [
    "MAX_NUM_WORDS = 20000\n",
    "tokenizer = Tokenizer(num_words = MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(documents_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "print('vocab_size: ' + str(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(documents_train)\n",
    "X_test = tokenizer.texts_to_sequences(documents_test)\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = create_embedding_matrix('resource/glove.6B.50d.txt', tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab overlap: 0.4319623158519188\n"
     ]
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "overlap = nonzero_elements / vocab_size\n",
    "print('Vocab overlap: ' + str(overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 174074 unique tokens.\n",
      "Shape of data tensor: (19997, 1000)\n",
      "Shape of label tensor: (19997, 20)\n"
     ]
    }
   ],
   "source": [
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "# tokenizing the text and creating vecotr tensor out of that\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# vetorizing the text samples into 2D integer tensor \n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "# pad sequences\n",
    "# MAX_SEQUENCE_LENGTH = max([len(s.split()) for s in texts])\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(EMBEDDING_DIR, EMBEDDING_FILE_NAME), encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n"
     ]
    }
   ],
   "source": [
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "# preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "preds = Dense(20, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15998 samples, validate on 3999 samples\n",
      "Epoch 1/10\n",
      "  128/15998 [..............................] - ETA: 22:15 - loss: 3.1493 - acc: 0.0547"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-5a6fdff2407a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m           validation_data=(x_val, y_val))\n\u001b[0m",
      "\u001b[1;32mc:\\users\\52028512\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\52028512\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\52028512\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\52028512\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\52028512\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
